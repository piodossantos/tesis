{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cItRFldQwn02",
        "outputId": "e2afdb74-4eeb-4768-97d3-e2661d39551b"
      },
      "outputs": [],
      "source": [
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "import os\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, img_dir):\n",
        "        self.img_dir = img_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        _, _, files = next(os.walk(self.img_dir))\n",
        "        file_count = len(files)\n",
        "        return file_count\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        _, _, files = next(os.walk(self.img_dir))\n",
        "        img_path = files[idx]\n",
        "        image = read_image(f'{self.img_dir}/{img_path}').float()\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((640, 640)),\n",
        "        ])\n",
        "        image = transform(image)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of the dataset is: 66261\n",
            "length of the train dataset is: 46383\n",
            "length of the val dataset is: 13252\n",
            "length of the test dataset is: 6626\n"
          ]
        }
      ],
      "source": [
        "mask_dataset = CustomImageDataset('./masks')\n",
        "print(\"length of the dataset is:\", len(mask_dataset))\n",
        "\n",
        "train, val, test = random_split(mask_dataset, [0.7, 0.2, 0.1])\n",
        "\n",
        "print(\"length of the train dataset is:\", len(train))\n",
        "print(\"length of the val dataset is:\", len(val))\n",
        "print(\"length of the test dataset is:\", len(test))\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "train_loader = DataLoader(val, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import flatten, reshape\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 160, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(160, 80, 5)\n",
        "        self.conv3 = nn.Conv2d(80, 40, 5)\n",
        "        self.conv4 = nn.Conv2d(40, 20, 5)\n",
        "        self.conv5 = nn.Conv2d(20, 10, 5)\n",
        "        self.conv6 = nn.Conv2d(10, 5, 5)\n",
        "        self.fc3 = nn.Linear(180, 64)\n",
        "        self.upsample = nn.UpsamplingNearest2d(scale_factor=2)\n",
        "        self.unf = nn.Linear(64, 180)\n",
        "        self.unconv1 = nn.ConvTranspose2d(5, 10, 5)\n",
        "        self.unconv2 = nn.ConvTranspose2d(10, 20, 5)\n",
        "        self.unconv3 = nn.ConvTranspose2d(20, 40, 5)\n",
        "        self.unconv4 = nn.ConvTranspose2d(40, 80, 5)\n",
        "        self.unconv5 = nn.ConvTranspose2d(80, 160, 5)\n",
        "        self.unconv6 = nn.ConvTranspose2d(160, 4, 5)\n",
        "        self.finalUpsample = nn.UpsamplingNearest2d((636, 636))\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = self.pool((self.conv1(x)))\n",
        "        x = self.pool((self.conv2(x)))\n",
        "        x = self.pool((self.conv3(x)))\n",
        "        x = self.pool((self.conv4(x)))\n",
        "        x = self.pool((self.conv5(x)))\n",
        "        x = self.pool((self.conv6(x)))\n",
        "        x = flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x):\n",
        "        x = self.unf(x)\n",
        "        x = reshape(x, (-1, 5, 6, 6))\n",
        "        x = self.unconv1(self.upsample(x))\n",
        "        x = self.unconv2(self.upsample(x))\n",
        "        x = self.unconv3(self.upsample(x))\n",
        "        x = self.unconv4(self.upsample(x))\n",
        "        x = self.unconv5(self.upsample(x))\n",
        "        x = self.unconv6(self.finalUpsample(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1        [-1, 160, 636, 636]          16,160\n",
            "         MaxPool2d-2        [-1, 160, 318, 318]               0\n",
            "            Conv2d-3         [-1, 80, 314, 314]         320,080\n",
            "         MaxPool2d-4         [-1, 80, 157, 157]               0\n",
            "            Conv2d-5         [-1, 40, 153, 153]          80,040\n",
            "         MaxPool2d-6           [-1, 40, 76, 76]               0\n",
            "            Conv2d-7           [-1, 20, 72, 72]          20,020\n",
            "         MaxPool2d-8           [-1, 20, 36, 36]               0\n",
            "            Conv2d-9           [-1, 10, 32, 32]           5,010\n",
            "        MaxPool2d-10           [-1, 10, 16, 16]               0\n",
            "           Conv2d-11            [-1, 5, 12, 12]           1,255\n",
            "        MaxPool2d-12              [-1, 5, 6, 6]               0\n",
            "           Linear-13                   [-1, 64]          11,584\n",
            "           Linear-14                  [-1, 180]          11,700\n",
            "UpsamplingNearest2d-15            [-1, 5, 12, 12]               0\n",
            "  ConvTranspose2d-16           [-1, 10, 16, 16]           1,260\n",
            "UpsamplingNearest2d-17           [-1, 10, 32, 32]               0\n",
            "  ConvTranspose2d-18           [-1, 20, 36, 36]           5,020\n",
            "UpsamplingNearest2d-19           [-1, 20, 72, 72]               0\n",
            "  ConvTranspose2d-20           [-1, 40, 76, 76]          20,040\n",
            "UpsamplingNearest2d-21         [-1, 40, 152, 152]               0\n",
            "  ConvTranspose2d-22         [-1, 80, 156, 156]          80,080\n",
            "UpsamplingNearest2d-23         [-1, 80, 312, 312]               0\n",
            "  ConvTranspose2d-24        [-1, 160, 316, 316]         320,160\n",
            "UpsamplingNearest2d-25        [-1, 160, 636, 636]               0\n",
            "  ConvTranspose2d-26          [-1, 4, 640, 640]          16,004\n",
            "================================================================\n",
            "Total params: 908,413\n",
            "Trainable params: 908,413\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 6.25\n",
            "Forward/backward pass size (MB): 1414.77\n",
            "Params size (MB): 3.47\n",
            "Estimated Total Size (MB): 1424.49\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "decoder = AutoEncoder().to(device)\n",
        "summary(decoder, (4,640, 640,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Adam' object has no attribute 'to'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Adam' object has no attribute 'to'"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss().to(device)\n",
        "optimizer = optim.Adam(decoder.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mmandirola/tesis/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs = data.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = decoder(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
