{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cleanlab \"cleanlab[image]\" datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "def load_embedding(path):\n",
    "    with open(path, newline='\\n') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "        embeddings = []\n",
    "        for row in spamreader:\n",
    "            array = np.array(np.float32(row))\n",
    "            embeddings.append(array)\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "embeddings = load_embedding('yoloV8-9b683e5d1039c81f2747a707f9c7f24a86c64a22c1464c6393db1538d42ca634.tsv')\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.outlier import OutOfDistribution\n",
    "\n",
    "ood = OutOfDistribution()\n",
    "\n",
    "ood_train_feature_scores = ood.fit_score(features=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(ood_train_feature_scores, range=[0,1], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siameseTrain = load_embedding('siameseResnetTrain.tsv')\n",
    "print(siameseTrain.shape)\n",
    "\n",
    "siameseTest = load_embedding('siameseResnetTest.tsv')\n",
    "print(siameseTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ood = OutOfDistribution()\n",
    "\n",
    "ood_train_feature_scores = ood.fit_score(features=siameseTrain)\n",
    "ood_test_feature_scores = ood.score(features=siameseTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifth_percentile = np.percentile(ood_train_feature_scores, 5)  # 5th percentile of the train_data distribution\n",
    "\n",
    "# Plot outlier_score distributions and the 5th percentile cutoff\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "plt_range = [min(ood_train_feature_scores.min(),ood_test_feature_scores.min()), \\\n",
    "             max(ood_train_feature_scores.max(),ood_test_feature_scores.max())]\n",
    "axes[0].hist(ood_train_feature_scores, range=plt_range, bins=50)\n",
    "axes[0].set(title='train_outlier_scores distribution', ylabel='Frequency')\n",
    "axes[0].axvline(x=fifth_percentile, color='red', linewidth=2)\n",
    "axes[1].hist(ood_test_feature_scores, range=plt_range, bins=50)\n",
    "axes[1].set(title='test_outlier_scores distribution', ylabel='Frequency')\n",
    "axes[1].axvline(x=fifth_percentile, color='red', linewidth=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "class ImagePairsDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.features = set()\n",
    "        self.data = self.load_data(f'{root_dir}/{csv_file}')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (img1_path, _, interval1) = self.data[idx]\n",
    "        # Load images\n",
    "        img1 = Image.open(os.path.join(self.root_dir, img1_path)).convert('RGB')\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "\n",
    "        return img1, interval1\n",
    "\n",
    "    def load_data(self, csv_file):\n",
    "        data = []\n",
    "        with open(csv_file, 'r') as file:\n",
    "            for line in file:\n",
    "                filename, video, interval = line.strip().split(',')\n",
    "                data.append((filename, video, interval))\n",
    "                self.features.add(interval)\n",
    "        return data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 360)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ImagePairsDataset('tag.csv', 'siamese', transform=transform)\n",
    "num_classes = len(dataset.features)\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def gen_hf_dataset():\n",
    "    for image, label in dataset:\n",
    "        yield {'image': image, 'label': label}\n",
    "\n",
    "\n",
    "hf_dataset = Dataset.from_generator(gen_hf_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNet\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNet\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[0;32m~/tesis/venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/tesis/venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tesis/venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/tesis/venv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.linear = nn.Sequential(nn.LazyLinear(128), nn.ReLU())\n",
    "        self.output = nn.Sequential(nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def embeddings(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h7/l7ypw92j0g57hsv58b90t1r00000gn/T/ipykernel_92850/3318949670.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from torch import optim\n",
    "import time\n",
    "\n",
    "# Method to calculate validation accuracy in each epoch\n",
    "\n",
    "def get_test_accuracy(net, testloader):\n",
    "    net.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = net(images)\n",
    "\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = 100 * accuracy / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Method for training the model\n",
    "def train(trainloader, testloader, n_epochs, patience):\n",
    "    model = Net()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    best_test_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        start_epoch = time.time()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for _, data in enumerate(trainloader):\n",
    "            # get the inputs; data is a dict of {\"image\": images, \"label\": labels}\n",
    "\n",
    "            inputs, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.detach().cpu().item()\n",
    "\n",
    "        # Get accuracy on the test set\n",
    "        accuracy = get_test_accuracy(model, testloader)\n",
    "\n",
    "        if accuracy > best_test_accuracy:\n",
    "            best_epoch = epoch\n",
    "\n",
    "        # Condition for early stopping\n",
    "        if epoch - best_epoch > patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        end_epoch = time.time()\n",
    "\n",
    "        print(\n",
    "            f\"epoch: {epoch + 1} loss: {running_loss / len(trainloader):.3f} test acc: {accuracy:.3f} time_taken: {end_epoch - start_epoch:.3f}\"\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Method for computing out-of-sample embeddings\n",
    "def compute_embeddings(model, testloader):\n",
    "    embeddings_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "\n",
    "            embeddings = model.embeddings(images)\n",
    "            embeddings_list.append(embeddings.cpu())\n",
    "\n",
    "    return torch.vstack(embeddings_list)\n",
    "\n",
    "\n",
    "# Method for computing out-of-sample predicted probabilities\n",
    "def compute_pred_probs(model, testloader):\n",
    "    pred_probs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data[\"image\"].to(device), data[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            pred_probs_list.append(outputs.cpu())\n",
    "\n",
    "    return torch.vstack(pred_probs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([   0,    1,    3, ..., 9119, 9120, 9122]), array([   0,    2,    4, ..., 9119, 9120, 9121]), array([   1,    2,    3, ..., 9117, 9121, 9122])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmandirola/tesis/venv/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "K = 3  # Number of cross-validation folds. Set to small value here to ensure quick runtimes, we recommend 5 or 10 in practice for more accurate estimates.\n",
    "n_epochs = 2  # Number of epochs to train model for. Set to a small value here for quick runtime, you should use a larger value in practice.\n",
    "patience = 2  # Parameter for early stopping. If the validation accuracy does not improve for this many epochs, training will stop.\n",
    "train_batch_size = 64  # Batch size for training\n",
    "test_batch_size = 512  # Batch size for testing\n",
    "num_workers = multiprocessing.cpu_count()  # Number of workers for data loaders\n",
    "\n",
    "# Create k splits of the dataset\n",
    "kfold = StratifiedKFold(n_splits=K, shuffle=True)\n",
    "splits = kfold.split(hf_dataset, hf_dataset['label'])\n",
    "\n",
    "train_id_list, test_id_list = [], []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(splits):\n",
    "    train_id_list.append(train_ids)\n",
    "    test_id_list.append(test_ids)\n",
    "\n",
    "print(train_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Subset' from 'torch.utils' (/Users/mmandirola/tesis/venv/lib/python3.10/site-packages/torch/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Subset, DataLoader\n\u001b[1;32m      3\u001b[0m pred_probs_list, embeddings_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m      4\u001b[0m embeddings_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Subset' from 'torch.utils' (/Users/mmandirola/tesis/venv/lib/python3.10/site-packages/torch/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "pred_probs_list, embeddings_list = [], []\n",
    "embeddings_model = None\n",
    "\n",
    "for i in range(K):\n",
    "    print(f\"\\nTraining on fold: {i+1} ...\")\n",
    "\n",
    "    # Create train and test sets and corresponding dataloaders\n",
    "    trainset = Subset(hf_dataset, train_id_list[i])\n",
    "    testset = Subset(hf_dataset, test_id_list[i])\n",
    "\n",
    "    trainloader = DataLoader(\n",
    "        trainset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    testloader = DataLoader(\n",
    "        testset, batch_size=test_batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model = train(trainloader, testloader, n_epochs, patience)\n",
    "    if embeddings_model is None:\n",
    "        embeddings_model = model\n",
    "\n",
    "    # Compute out-of-sample embeddings\n",
    "    print(\"Computing feature embeddings ...\")\n",
    "    fold_embeddings = compute_embeddings(embeddings_model, testloader)\n",
    "    embeddings_list.append(fold_embeddings)\n",
    "\n",
    "    print(\"Computing predicted probabilities ...\")\n",
    "    # Compute out-of-sample predicted probabilities\n",
    "    fold_pred_probs = compute_pred_probs(model, testloader)\n",
    "    pred_probs_list.append(fold_pred_probs)\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "\n",
    "# Combine embeddings and predicted probabilities from each fold\n",
    "features = torch.vstack(embeddings_list).numpy()\n",
    "\n",
    "logits = torch.vstack(pred_probs_list)\n",
    "pred_probs = nn.Softmax(dim=1)(logits).numpy()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
